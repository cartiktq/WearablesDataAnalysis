# WearablesDataAnalysis

This project is an approximation of some of my most recent work. The actual codebase is copyright protected by my former employers. The project details a training workflow for an unsupervised learning algorithm. It contains pseudocode, for a collection of Python classes, each of which can be containerized to perform the following steps of the workflow:

1) Python class 1 uses agentic API to extract data in CSV format from a AWS Kinesis or Kafka stream supplying real-time wearables data, containing variables including subject identifiers (generated identifiers not PHI such as SSN's or MRN's), for heart rate, blood pressure, surface skin temperature, oxygen saturation, and pulse rate,
2) Python class 2, which collects real-time weather data such as temperature, heat index, wind chill, and humidity from weather sources, 
3) Python class 3 uses agentic API to extract data in CSV format from a static database (noSQL like DynamoDB) containing subject demographics data such as age, sex, gender, and ethnicity, physical parameters such as height, weight, and body surface area, for the same cohort of subjects as in (1) above
4) Python class 4 uses agentic API to extract data in CSV format from a clinical data warehouse in Amazon Redshift that contains  information such as diagnosis, prescriptions, lab test results, and invokes a separate agent (Python class 4a using Agentic API), which uses an LLM API such as OpenAI API to summarize rich-text clinical notes and extracts clinical codes such as ICD-9/10, SNOMED CT, RxNORM, and CPT codes from the notes, for the same cohort of subjects as in (1) above. All of this information is collated and outputted in CSV format
5) Python Class 5, which implements an appropriate clustering algorithm, after performing variable selection (not PCA) that uses the parameters extracted from two information sources: DEMOGRAPHICS AND CLINICAL DATA (extracted by Agents 2 and 3 and 3a) to stratify the subject cohort
6) Python Class 6, which evaluates the performance of the clustering algorithm and performs hyperparameter tuning and stops the ML training when the performance metric (F-score) levels off 
7) Python Class 7, which outputs the details of the clusters generated by Python Class 4, 
8) Python Class 8, which uses the T-SNE algorithm and the Seaborn package to create and display a visualization of the generated clusters in three dimensions
9) Python Class 9, which uses an LLM API, such as OpenAI API or Gemini, to look at the patient parameters of the subject cohorts in each of the clusters, to generate a text summary describing the unique characters of each group

As mentioned before, each class can be containerized with Docker, which can then be composed into an orchestrated workflow implemented in Kubernetes

A) This is only one part (about 20%) in the process of training a ML model on unlabeled data to detect patterns in it
B) The next step would involve training multiple ML models using: i)  logistic regressor with L1 regularization, ii) a random forest, iii) a boosting algorithm, and iv) a support vector machine to partition the live-stream wearables data generated by Python Classes 1 and 2 (see above) into the separate patient strata to predict an adverse event such as fatigue, heat exhaustion, or hypothermia and after multiple rounds of training, validation, hyperparameters adjustment, and testing, the selection of the best performing model for live deployment
C) The last step would be the ML deployment in an A/B deployment setup, where the best performing model (the champion) is on the A-channel and the next best performing model (the challenger) is on the B-channel. Both models generate predictions on live-streaming data feeds from the wearables and the weather information source. Their performance is monitored with a tool like Grafana. Since minimizing false positives is not a concern (better to address a suspected adverse event rather than ignore), the performance metric that will be used here is recall

